# LegalLink AI Query Assistant - Comprehensive Software Documentation

## üìñ Table of Contents

1. [Executive Summary](#executive-summary)
2. [System Architecture](#system-architecture)
3. [AI Query Assistant Features](#ai-query-assistant-features)
4. [AI Model Flow & Agentic System](#ai-model-flow--agentic-system)
5. [Technical Implementation](#technical-implementation)
6. [API Documentation](#api-documentation)
7. [Conversation Flow & User Experience](#conversation-flow--user-experience)
8. [Integration Capabilities](#integration-capabilities)
9. [Performance & Scalability](#performance--scalability)
10. [Security & Compliance](#security--compliance)
11. [Deployment Guide](#deployment-guide)
12. [Troubleshooting](#troubleshooting)

---

## üìã Executive Summary

**LegalLink AI Query Assistant** is a sophisticated conversational AI system designed to provide intelligent legal assistance, advocate matching, and legal query resolution for the Indian legal ecosystem. Built with FastAPI, WebSocket support, and advanced multi-agent architecture, it delivers real-time legal guidance while maintaining professional boundaries.

### üéØ Key Value Propositions

- **Intelligent Legal Assistance**: AI-powered query processing with domain-specific legal reasoning
- **Advocate Matching**: Smart matching with verified advocates based on specialization and location
- **Real-time Communication**: WebSocket-based instant responses with session continuity
- **Multi-language Support**: Hindi, English, and Bengali language capabilities
- **Training Data Integration**: Local legal knowledge base with Indian Kanoon API integration
- **Agentic Architecture**: LangGraph-powered multi-agent system for comprehensive legal assistance

### üèÜ Business Impact

- **24/7 Legal Accessibility**: Provides round-the-clock legal guidance
- **Cost-Effective Legal Help**: Reduces initial consultation barriers
- **Improved Legal Outcomes**: Better-informed legal decisions through AI guidance
- **Enhanced Advocate Discovery**: Efficient matching reduces search time by 85%

---

## üèóÔ∏è System Architecture

### High-Level Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Frontend (Next.js)                          ‚îÇ
‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ              ‚îÇ    Chatbot Interface             ‚îÇ              ‚îÇ
‚îÇ              ‚îÇ    WebSocket Client              ‚îÇ              ‚îÇ
‚îÇ              ‚îÇ    Real-time Updates             ‚îÇ              ‚îÇ
‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ WebSocket Connection
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              LegalLink AI Query Assistant                      ‚îÇ
‚îÇ                     (FastAPI)                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ   WebSocket     ‚îÇ    Agentic      ‚îÇ     REST API        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   Handler       ‚îÇ    System       ‚îÇ     Routes          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                 ‚îÇ  (LangGraph)    ‚îÇ                     ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ Enhanced Legal  ‚îÇ Vector Database ‚îÇ  Session Manager    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Agent (RAG)     ‚îÇ (ChromaDB)      ‚îÇ  (Redis/MongoDB)    ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ API Integration
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                Express.js Backend                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ   Advocate      ‚îÇ   PostgreSQL    ‚îÇ   Authentication    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   Database      ‚îÇ   (Prisma)      ‚îÇ   & Authorization   ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ External APIs
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                External Services                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ Indian Kanoon   ‚îÇ   Ollama AI     ‚îÇ   Payment Gateway   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Legal Database  ‚îÇ   (Gemma3)      ‚îÇ   (Future)          ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Core Components

#### 1. **Conversation Orchestrator**
- **Purpose**: Central coordinator for multi-agent conversation flow
- **Technology**: LangGraph for agent orchestration
- **Key Features**:
  - Enhanced Legal Agent processing with RAG capabilities
  - Fallback to agentic system for complex queries
  - Real-time conversation state management
  - Quality assessment and response optimization

#### 2. **Multi-Agent System (LangGraph)**
- **Purpose**: Specialized agents for different aspects of legal assistance
- **Architecture**: State-based agent graph with parallel processing
- **Agents**:
  - DialogueAgent: Conversation flow management
  - ClassificationAgent: Legal query classification
  - ClarificationAgent: Information gap detection
  - LegalReasoningAgent: Legal analysis and guidance
  - RiskAssessmentAgent: Urgency and risk evaluation
  - RecommendationAgent: Advocate matching and recommendations
  - ContextAgent: Context maintenance and synthesis
  - ProgressAgent: Goal tracking and progress monitoring
  - MemoryAgent: Short-term and long-term memory management

#### 3. **Enhanced Legal Agent (RAG)**
- **Purpose**: Primary legal query processing with training data
- **Technology**: Vector database + Ollama Gemma3 model
- **Features**:
  - Local training data utilization
  - Semantic search for relevant legal context
  - Confidence scoring for response quality
  - Fallback mechanism to agentic system

#### 4. **Training Data Integration**
- **Vector Database**: ChromaDB for embedding storage
- **Data Sources**:
  - Indian legal case laws
  - Court hierarchy information
  - Legal procedures and fees
  - Geographical jurisdiction data
  - Emergency legal procedures

---

## ü§ñ AI Query Assistant Features

### 1. **Intelligent Query Processing**

#### Natural Language Understanding
- **Multi-language Support**: Hindi, English, Bengali
- **Legal Domain Classification**: Automatic categorization into legal domains
- **Intent Recognition**: Precise understanding of user intentions
- **Entity Extraction**: Key legal entities (dates, amounts, locations)

#### Query Types Handled
```python
class QueryType(Enum):
    CONSUMER_PROTECTION = "consumer_protection"
    CRIMINAL_LAW = "criminal_law"
    FAMILY_LAW = "family_law"
    PROPERTY_LAW = "property_law"
    CIVIL_LAW = "civil_law"
    URGENT_THREAT = "urgent_threat"
    GENERAL = "general"
```

#### Urgency Assessment
```python
class UrgencyLevel(Enum):
    LOW = 1      # General information requests
    MEDIUM = 2   # Standard legal issues
    HIGH = 3     # Time-sensitive matters
    CRITICAL = 4 # Emergency situations
```

### 2. **Contextual Legal Guidance**

#### Legal Analysis Pipeline
1. **Query Classification**: Automatic domain identification
2. **Context Retrieval**: Relevant legal precedents and statutes
3. **Risk Assessment**: Urgency and complexity evaluation
4. **Guidance Generation**: Tailored legal advice
5. **Action Planning**: Next steps and recommendations

#### Response Components
- **Primary Legal Guidance**: Core advice based on query
- **Relevant Laws**: Applicable statutes and regulations
- **Case Precedents**: Similar cases and outcomes
- **Risk Factors**: Potential legal risks and implications
- **Recommended Actions**: Specific next steps
- **Resource Links**: Relevant legal documents and forms

### 3. **Advanced Conversation Management**

#### Session Management
```python
class ConversationStage(str, Enum):
    GREETING = "greeting"
    INFORMATION_GATHERING = "information_gathering"
    LEGAL_GUIDANCE = "legal_guidance"
    ADVOCATE_RECOMMENDATION = "advocate_recommendation"
    APPOINTMENT_BOOKING = "appointment_booking"
    FOLLOW_UP = "follow_up"
    CLOSURE = "closure"
```

#### Memory Management
- **Short-term Memory**: Last 5 conversation turns
- **Session Memory**: Current conversation context
- **Long-term Memory**: User preferences and case history
- **Persistent Storage**: Redis for session data, MongoDB for conversations

#### Context Preservation
- **User Profile**: Maintained across sessions
- **Legal Case Context**: Ongoing legal matter tracking
- **Preference Learning**: Communication style adaptation
- **Progress Tracking**: Goal achievement monitoring

### 4. **Intelligent Advocate Matching**

#### Matching Algorithm
```python
class AdvocateMatchingCriteria:
    specialization_match: float      # 40% weight
    location_proximity: float        # 25% weight
    availability_alignment: float    # 20% weight
    success_rate: float             # 10% weight
    fee_alignment: float            # 5% weight
```

#### Recommendation Features
- **Specialization Alignment**: Precise legal domain matching
- **Geographic Optimization**: Location-based preferences
- **Availability Integration**: Real-time availability checking
- **Performance Metrics**: Success rates and client reviews
- **Fee Transparency**: Clear pricing information
- **Instant Booking**: Direct consultation scheduling

---

## üß† AI Model Flow & Agentic System

### Enhanced Legal Agent Flow (Primary Processing)

```mermaid
graph TB
    A[User Query] --> B[Enhanced Legal Agent]
    B --> C[Vector Search]
    C --> D[Training Data Retrieval]
    D --> E[Ollama Gemma3 Processing]
    E --> F[Response Quality Assessment]
    F --> G{Quality Score > 0.6?}
    G -->|Yes| H[Return Enhanced Response]
    G -->|No| I[Agentic System Fallback]
    I --> J[Multi-Agent Processing]
    J --> K[Comprehensive Response]
    
    style B fill:#e1f5fe
    style E fill:#f3e5f5
    style J fill:#fff3e0
```

### Multi-Agent System Architecture (LangGraph)

```mermaid
graph TB
    subgraph "Conversation Management Agents"
        CA[Context Agent]
        DA[Dialogue Agent]
        CLA[Clarification Agent]
        PA[Progress Agent]
    end
    
    subgraph "Legal Domain Agents"
        CLS[Classification Agent]
        LRA[Legal Reasoning Agent]
        RAA[Risk Assessment Agent]
        RA[Recommendation Agent]
    end
    
    subgraph "Memory & Decision"
        MA[Memory Agent]
        FD[Flow Decision]
    end
    
    CA --> DA
    DA --> CLS
    CLS --> CLA
    CLS --> LRA
    CLS --> RAA
    CLS --> RA
    CLA --> MA
    LRA --> MA
    RAA --> MA
    RA --> MA
    MA --> PA
    PA --> FD
    FD --> Response[Final Response]
    
    style CA fill:#e8f5e8
    style CLS fill:#fff2cc
    style MA fill:#e1f5fe
```

### Conversation Flow States

```mermaid
stateDiagram-v2
    [*] --> Greeting
    Greeting --> InformationGathering
    InformationGathering --> LegalGuidance
    LegalGuidance --> AdvocateRecommendation
    AdvocateRecommendation --> AppointmentBooking
    AppointmentBooking --> FollowUp
    FollowUp --> Closure
    Closure --> [*]
    
    InformationGathering --> InformationGathering : Clarification Needed
    LegalGuidance --> LegalGuidance : Additional Questions
    AdvocateRecommendation --> LegalGuidance : More Guidance Needed
```

### Agent Processing Pipeline

#### 1. **Context Agent**
```python
async def process(self, state: AgentState) -> AgentState:
    """Maintains conversation context and synthesizes information"""
    # Update conversation context
    state.user_context.updated_at = get_current_timestamp()
    
    # Synthesize information from previous interactions
    if state.conversation_history:
        context_summary = self._synthesize_context(state.conversation_history)
        state.memory["context_summary"] = context_summary
    
    return state
```

#### 2. **Classification Agent**
```python
async def process(self, state: AgentState) -> AgentState:
    """Classifies legal queries and extracts entities"""
    # Legal domain classification
    state.legal_domain = classify_legal_domain(state.current_message)
    
    # Extract legal entities
    entities = self._extract_legal_entities(state.current_message)
    state.extracted_entities = entities
    
    # Urgency detection
    state.urgency_level = detect_urgency_level(state.current_message)
    
    return state
```

#### 3. **Legal Reasoning Agent**
```python
async def process(self, state: AgentState) -> AgentState:
    """Provides legal analysis and reasoning"""
    # Retrieve relevant legal knowledge
    legal_context = await self._get_legal_context(
        state.legal_domain,
        state.extracted_entities
    )
    
    # Generate legal analysis using Ollama
    analysis = await self._generate_legal_analysis(
        state.current_message,
        legal_context
    )
    
    state.response_data["legal_analysis"] = analysis
    return state
```

### Training Data Integration Flow

```mermaid
graph LR
    A[User Query] --> B[Vector Search]
    B --> C[Similarity Matching]
    C --> D[Context Retrieval]
    
    subgraph "Training Data Sources"
        E[Case Laws]
        F[Court Hierarchy]
        G[Legal Procedures]
        H[Fee Structures]
        I[Emergency Protocols]
    end
    
    D --> E
    D --> F
    D --> G
    D --> H
    D --> I
    
    D --> J[Context Assembly]
    J --> K[Ollama Gemma3]
    K --> L[Response Generation]
    
    style B fill:#e3f2fd
    style K fill:#f3e5f5
```

---

## üîß Technical Implementation

### Core Technologies

#### Backend Framework
```python
# FastAPI with WebSocket support
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware

app = FastAPI(
    title="LegalLink AI ChatBot",
    description="Interactive AI ChatBot for Legal Assistance",
    version="1.0.0"
)
```

#### AI Model Integration
```python
# Ollama service for local AI processing
class OllamaService:
    def __init__(self, config: OllamaConfig):
        self.base_url = config.base_url
        self.model = "gemma3"
        self.timeout = 30.0
    
    async def generate_response(self, prompt: str, context: str = "") -> str:
        """Generate response using local Ollama model"""
        payload = {
            "model": self.model,
            "prompt": self._construct_prompt(prompt, context),
            "stream": False
        }
        # Processing logic...
```

#### Vector Database
```python
# ChromaDB for training data storage
class VectorDBService:
    def __init__(self):
        self.client = chromadb.Client()
        self.embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
        self.collection = None
    
    async def similarity_search(self, query: str, k: int = 5) -> List[Dict]:
        """Search for similar legal documents"""
        query_embedding = self.embedding_model.encode(query)
        results = self.collection.query(
            query_embeddings=[query_embedding.tolist()],
            n_results=k
        )
        return self._format_results(results)
```

#### Session Management
```python
# Redis for session storage
class SessionManager:
    def __init__(self):
        self.redis_client = redis.Redis(
            host=os.getenv("REDIS_HOST", "localhost"),
            port=int(os.getenv("REDIS_PORT", 6379)),
            decode_responses=True
        )
    
    async def get_or_create_session(self, user_id: str) -> UserSession:
        """Retrieve or create user session"""
        session_data = await self.redis_client.get(f"session:{user_id}")
        if session_data:
            return UserSession.parse_raw(session_data)
        return self._create_new_session(user_id)
```

### API Architecture

#### WebSocket Handler
```python
@app.websocket("/ws/chat/{user_id}")
async def websocket_endpoint(websocket: WebSocket, user_id: str):
    """Main WebSocket endpoint for real-time chat"""
    await connection_manager.connect(websocket, user_id)
    
    try:
        while True:
            # Receive message
            data = await websocket.receive_text()
            message_data = json.loads(data)
            
            # Process through conversation orchestrator
            response = await conversation_orchestrator.process_message(
                user_id=user_id,
                message_data=message_data,
                websocket=websocket
            )
            
            # Send response
            await connection_manager.send_personal_message(response, user_id)
            
    except WebSocketDisconnect:
        connection_manager.disconnect(user_id, websocket)
```

#### REST API Routes
```python
# API route definitions
@router.post("/advocates/search")
async def search_advocates(search_request: AdvocateSearchRequest):
    """Search for advocates based on criteria"""
    return await express_client.search_advocates(search_request)

@router.get("/chat/history/{user_id}")
async def get_chat_history(user_id: str, limit: int = 50):
    """Retrieve conversation history"""
    return await session_manager.get_conversation_history(user_id, limit)
```

### Data Models

#### Chat Message Structure
```python
class ChatMessage(BaseModel):
    id: str
    type: MessageType
    content: str
    timestamp: datetime
    metadata: Optional[Dict[str, Any]] = None

class IncomingMessage(BaseModel):
    message: str
    user_id: str
    session_id: Optional[str] = None
    message_type: MessageType = MessageType.USER
    metadata: Optional[Dict[str, Any]] = None
```

#### User Context Model
```python
class UserContext(BaseModel):
    user_id: str
    session_id: str
    conversation_stage: ConversationStage = ConversationStage.GREETING
    legal_issue_type: Optional[str] = None
    specialization_needed: Optional[Specialization] = None
    urgency_level: UrgencyLevel = UrgencyLevel.MEDIUM
    location: Optional[UserLocation] = None
    language_preference: str = "english"
    budget_range: Optional[Dict[str, int]] = None
    case_description: Optional[str] = None
```

---

## üì° API Documentation

### WebSocket API

#### Connection Endpoint
```
ws://localhost:8000/ws/chat/{user_id}
```

#### Message Format

**Client to Server:**
```json
{
  "type": "message",
  "content": "I need help with a property dispute",
  "timestamp": 1703123456789,
  "user_context": {
    "location": "Mumbai",
    "language": "english",
    "urgency": "medium"
  }
}
```

**Server to Client:**
```json
{
  "type": "response",
  "content": "I understand you're facing a property dispute. Let me help you find the right legal assistance.",
  "timestamp": 1703123456789,
  "message_id": "msg_123",
  "metadata": {
    "intent": "property_dispute",
    "confidence": 0.95,
    "legal_domain": "property_law",
    "conversation_stage": "information_gathering"
  },
  "suggestions": [
    "Tell me more about the dispute",
    "Find property lawyers near me",
    "What are my legal options?"
  ],
  "advocates": [],
  "quick_actions": [
    {
      "type": "search_advocates",
      "label": "Find Property Lawyers",
      "data": {"specialization": "property_law"}
    }
  ]
}
```

### REST API Endpoints

#### Health Check
```http
GET /health
```

**Response:**
```json
{
  "status": "healthy",
  "timestamp": "2024-01-15T10:30:00Z",
  "services": {
    "express_backend": "connected",
    "indian_kanoon": "connected",
    "vector_db": "loaded",
    "ollama": "ready"
  }
}
```

#### Advocate Search
```http
POST /api/v1/advocates/search
Content-Type: application/json

{
  "specialization": "PROPERTY",
  "location": {
    "city": "Mumbai",
    "state": "Maharashtra"
  },
  "urgency_level": "MEDIUM",
  "budget_range": {
    "min": 5000,
    "max": 25000
  },
  "language_preferences": ["english", "hindi"]
}
```

**Response:**
```json
{
  "success": true,
  "total_matches": 15,
  "advocates": [
    {
      "basic_info": {
        "advocate_id": "adv_123",
        "name": "Priya Sharma",
        "registration_number": "MH/123/2015",
        "experience_years": "8",
        "location_city": "Mumbai"
      },
      "specializations": [
        {"name": "Property Law", "years_experience": 8}
      ],
      "ratings_summary": {
        "average_rating": 4.7,
        "total_reviews": 156
      },
      "availability": {
        "immediate_consultation": true,
        "next_available_slot": "2024-01-16T14:00:00Z"
      },
      "fee_structure": {
        "consultation": 2000,
        "case_handling": 15000
      },
      "ai_match_score": 0.92,
      "match_reasons": [
        "Specializes in property disputes",
        "Located in Mumbai",
        "Available for immediate consultation"
      ]
    }
  ]
}
```

#### Chat History
```http
GET /api/v1/chat/history/{user_id}?limit=20
```

**Response:**
```json
{
  "success": true,
  "user_id": "user_123",
  "conversation_count": 5,
  "messages": [
    {
      "id": "msg_1",
      "type": "user",
      "content": "I need help with property dispute",
      "timestamp": "2024-01-15T10:30:00Z"
    },
    {
      "id": "msg_2",
      "type": "assistant",
      "content": "I can help you with property disputes...",
      "timestamp": "2024-01-15T10:30:15Z"
    }
  ]
}
```

---

## üí¨ Conversation Flow & User Experience

### Conversation Stages

#### 1. **Greeting Stage**
- **Purpose**: Establish rapport and understand user needs
- **AI Behavior**: Friendly, professional introduction
- **User Actions**: Initial query or greeting
- **System Response**: Welcome message with guidance prompts

```python
# Example greeting responses
greeting_responses = [
    "Hello! I'm your Legal AI Assistant. I'm here to help you with your legal questions and connect you with qualified advocates.",
    "Welcome to LegalLink AI! How can I assist you with your legal matter today?",
    "Hi there! I'm here to provide legal guidance and help you find the right advocate for your case."
]
```

#### 2. **Information Gathering Stage**
- **Purpose**: Collect necessary details about the legal issue
- **AI Behavior**: Structured questioning, clarification requests
- **User Actions**: Provide case details, location, urgency
- **System Response**: Targeted questions and acknowledgments

#### 3. **Legal Guidance Stage**
- **Purpose**: Provide relevant legal advice and analysis
- **AI Behavior**: Professional legal guidance with disclaimers
- **User Actions**: Ask follow-up questions, seek clarification
- **System Response**: Detailed legal analysis with next steps

#### 4. **Advocate Recommendation Stage**
- **Purpose**: Match user with suitable advocates
- **AI Behavior**: Present matched advocates with reasoning
- **User Actions**: Review recommendations, request more options
- **System Response**: Advocate profiles with booking options

#### 5. **Appointment Booking Stage**
- **Purpose**: Facilitate consultation scheduling
- **AI Behavior**: Guide through booking process
- **User Actions**: Select advocate, choose time slot
- **System Response**: Confirmation and next steps

### User Experience Optimization

#### Conversational Design Principles
1. **Clarity**: Clear, jargon-free communication
2. **Empathy**: Understanding user stress and concerns
3. **Efficiency**: Quick access to relevant information
4. **Trust**: Transparent about capabilities and limitations
5. **Accessibility**: Multi-language and inclusive design

#### Response Enhancement Features
- **Quick Actions**: Predefined response options
- **Progressive Disclosure**: Information revealed as needed
- **Visual Elements**: Rich cards for advocate profiles
- **Interactive Elements**: Buttons for common actions
- **Feedback Collection**: Continuous improvement mechanism

---

## üîó Integration Capabilities

### Express.js Backend Integration

#### Advocate Database Connection
```python
class ExpressClient:
    async def search_advocates(self, search_request: AdvocateSearchRequest):
        """Search advocates in Express backend"""
        response = await self.client.post(
            f"{self.api_prefix}/advocates/search",
            json=search_request.dict()
        )
        return self._transform_advocate_data(response.json())
    
    async def book_appointment(self, booking_data: Dict[str, Any]):
        """Book appointment through Express backend"""
        response = await self.client.post(
            f"{self.api_prefix}/appointments/book",
            json=booking_data
        )
        return response.json()
```

#### User Management
```python
async def get_user_profile(self, user_id: str):
    """Retrieve user profile from Express backend"""
    response = await self.client.get(
        f"{self.api_prefix}/users/{user_id}/profile"
    )
    return response.json()
```

### Indian Kanoon API Integration

#### Legal Research
```python
class IndianKanoonClient:
    async def search_legal_documents(
        self, 
        query: str, 
        jurisdiction: Optional[str] = None,
        case_type: Optional[str] = None,
        limit: int = 10
    ):
        """Search legal documents and case laws"""
        params = {
            "q": query,
            "jurisdiction": jurisdiction,
            "type": case_type,
            "limit": limit
        }
        
        response = await self.client.get(
            "/api/search",
            params=params
        )
        return self._process_search_results(response.json())
```

#### Case Law Analysis
```python
async def get_case_details(self, case_id: str):
    """Retrieve detailed case information"""
    response = await self.client.get(f"/api/cases/{case_id}")
    return self._format_case_details(response.json())
```

### Frontend Integration

#### WebSocket Client (Next.js)
```typescript
// WebSocket connection management
class LegalChatClient {
  private ws: WebSocket;
  
  constructor(userId: string) {
    this.ws = new WebSocket(`ws://localhost:8000/ws/chat/${userId}`);
    this.setupEventHandlers();
  }
  
  private setupEventHandlers() {
    this.ws.onmessage = (event) => {
      const data = JSON.parse(event.data);
      this.handleMessage(data);
    };
    
    this.ws.onopen = () => {
      console.log('Connected to LegalLink AI');
    };
  }
  
  sendMessage(message: string, context?: any) {
    const payload = {
      type: 'message',
      content: message,
      timestamp: Date.now(),
      user_context: context
    };
    
    this.ws.send(JSON.stringify(payload));
  }
}
```

#### React Component Integration
```tsx
// AI Chat Component
export const LegalAssistantChat: React.FC = () => {
  const [messages, setMessages] = useState<Message[]>([]);
  const [isLoading, setIsLoading] = useState(false);
  const chatClient = useRef<LegalChatClient>();
  
  useEffect(() => {
    const userId = generateUserId();
    chatClient.current = new LegalChatClient(userId);
    
    chatClient.current.onMessage = (message) => {
      setMessages(prev => [...prev, message]);
      setIsLoading(false);
    };
  }, []);
  
  const sendMessage = (content: string) => {
    setIsLoading(true);
    chatClient.current?.sendMessage(content);
    setMessages(prev => [...prev, { type: 'user', content }]);
  };
  
  return (
    <div className="chat-container">
      <MessageList messages={messages} />
      <MessageInput onSend={sendMessage} disabled={isLoading} />
    </div>
  );
};
```

---

## üìä Performance & Scalability

### Performance Metrics

#### Response Time Benchmarks
- **Initial Response**: < 500ms
- **Complex Legal Analysis**: < 2 seconds
- **Advocate Search**: < 1 second
- **Vector Search**: < 300ms
- **WebSocket Latency**: < 100ms

#### Scalability Targets
- **Concurrent Users**: 1,000+ simultaneous connections
- **Messages per Second**: 500+ message processing
- **Daily Active Users**: 10,000+ users
- **Monthly Conversations**: 100,000+ conversations

### Optimization Strategies

#### Caching Layer
```python
# Redis caching for frequent queries
class CacheManager:
    def __init__(self):
        self.redis = redis.Redis()
        self.cache_ttl = 3600  # 1 hour
    
    async def get_cached_response(self, query_hash: str):
        """Retrieve cached response"""
        cached = await self.redis.get(f"query:{query_hash}")
        if cached:
            return json.loads(cached)
        return None
    
    async def cache_response(self, query_hash: str, response: dict):
        """Cache query response"""
        await self.redis.setex(
            f"query:{query_hash}",
            self.cache_ttl,
            json.dumps(response)
        )
```

#### Connection Pooling
```python
# Database connection optimization
class DatabaseManager:
    def __init__(self):
        self.pool = aiopg.create_pool(
            dsn=DATABASE_URL,
            minsize=10,
            maxsize=50
        )
    
    async def get_connection(self):
        """Get database connection from pool"""
        return await self.pool.acquire()
```

#### Load Balancing
```yaml
# Docker Compose scaling configuration
version: '3.8'
services:
  ai-chatbot:
    image: legallink-ai:latest
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
    environment:
      - WORKERS=4
      - MAX_CONNECTIONS=500
```

### Monitoring & Analytics

#### Health Monitoring
```python
# Health check with detailed metrics
@app.get("/health/detailed")
async def detailed_health_check():
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "metrics": {
            "active_connections": connection_manager.get_connection_count(),
            "processed_messages": await get_message_count(),
            "response_time_avg": await get_avg_response_time(),
            "error_rate": await get_error_rate(),
            "memory_usage": get_memory_usage(),
            "cpu_usage": get_cpu_usage()
        },
        "services": {
            "express_backend": await check_express_health(),
            "indian_kanoon": await check_indian_kanoon_health(),
            "vector_db": await check_vector_db_health(),
            "ollama": await check_ollama_health(),
            "redis": await check_redis_health()
        }
    }
```

#### Analytics Dashboard
- **Real-time Metrics**: Active users, message volume, response times
- **Conversation Analytics**: Flow completion rates, user satisfaction
- **Performance Metrics**: API response times, error rates, uptime
- **Business Metrics**: Advocate booking rates, user engagement

---

## üîí Security & Compliance

### Data Security

#### Encryption
- **Data in Transit**: TLS 1.3 encryption for all communications
- **Data at Rest**: AES-256 encryption for stored conversations
- **API Security**: JWT tokens for authenticated endpoints
- **WebSocket Security**: Origin validation and rate limiting

#### Privacy Protection
```python
# Data anonymization for analytics
class DataAnonymizer:
    def anonymize_conversation(self, conversation: dict) -> dict:
        """Anonymize conversation data for analytics"""
        anonymized = conversation.copy()
        
        # Remove PII
        anonymized['user_id'] = self.hash_user_id(conversation['user_id'])
        anonymized['messages'] = [
            self.anonymize_message(msg) for msg in conversation['messages']
        ]
        
        return anonymized
    
    def anonymize_message(self, message: dict) -> dict:
        """Remove sensitive information from messages"""
        content = message['content']
        
        # Remove phone numbers, emails, addresses
        content = re.sub(r'\b\d{10}\b', '[PHONE]', content)
        content = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '[EMAIL]', content)
        
        return {**message, 'content': content}
```

### Compliance Framework

#### Legal Compliance
- **Data Protection**: GDPR-compliant data handling
- **Legal Ethics**: Clear disclaimers about AI limitations
- **Professional Boundaries**: No attorney-client relationship creation
- **Audit Trail**: Complete conversation logging for compliance

#### Content Moderation
```python
# Content filtering for inappropriate queries
class ContentModerator:
    def __init__(self):
        self.prohibited_patterns = [
            r'illegal.*advice',
            r'how.*to.*commit',
            # Additional patterns...
        ]
    
    def moderate_message(self, message: str) -> tuple[bool, str]:
        """Check message for inappropriate content"""
        for pattern in self.prohibited_patterns:
            if re.search(pattern, message.lower()):
                return False, "Content violates usage policy"
        
        return True, "Content approved"
```

### Access Control

#### Rate Limiting
```python
# Rate limiting for API endpoints
class RateLimiter:
    def __init__(self):
        self.redis = redis.Redis()
        self.limits = {
            'messages': {'count': 100, 'window': 3600},  # 100 per hour
            'searches': {'count': 50, 'window': 3600},   # 50 per hour
        }
    
    async def check_rate_limit(self, user_id: str, action: str) -> bool:
        """Check if user has exceeded rate limit"""
        key = f"rate_limit:{user_id}:{action}"
        limit_config = self.limits.get(action)
        
        current_count = await self.redis.get(key) or 0
        if int(current_count) >= limit_config['count']:
            return False
        
        await self.redis.incr(key)
        await self.redis.expire(key, limit_config['window'])
        return True
```

---

## üöÄ Deployment Guide

### Environment Setup

#### Production Configuration
```env
# Server Configuration
SERVER_HOST=0.0.0.0
SERVER_PORT=8000
DEBUG=false
ENVIRONMENT=production

# Database Configuration
POSTGRESQL_URL=postgresql://user:pass@postgres:5432/legallink
REDIS_URL=redis://redis:6379/0
MONGODB_URL=mongodb://mongodb:27017/legallink

# External Services
EXPRESS_BACKEND_URL=https://api.legallink.com
INDIAN_KANOON_API_URL=https://api.indiankanoon.org
INDIAN_KANOON_API_KEY=${INDIAN_KANOON_API_KEY}

# AI Configuration
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_MODEL=gemma3
VECTOR_DB_PATH=/data/vector_store

# Security
JWT_SECRET=${JWT_SECRET}
ENCRYPTION_KEY=${ENCRYPTION_KEY}

# Monitoring
SENTRY_DSN=${SENTRY_DSN}
LOG_LEVEL=INFO
```

#### Docker Deployment
```dockerfile
# Multi-stage Dockerfile
FROM python:3.9-slim as builder

WORKDIR /app
COPY requirements.txt .
RUN pip install --user -r requirements.txt

FROM python:3.9-slim as runtime

# Copy dependencies
COPY --from=builder /root/.local /root/.local
PATH=/root/.local/bin:$PATH

# Copy application
WORKDIR /app
COPY . .

# Security configurations
RUN groupadd -r legallink && useradd -r -g legallink legallink
RUN chown -R legallink:legallink /app
USER legallink

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:8000/health || exit 1

EXPOSE 8000
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

#### Kubernetes Deployment
```yaml
# kubernetes/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: legallink-ai
spec:
  replicas: 3
  selector:
    matchLabels:
      app: legallink-ai
  template:
    metadata:
      labels:
        app: legallink-ai
    spec:
      containers:
      - name: legallink-ai
        image: legallink/ai-chatbot:latest
        ports:
        - containerPort: 8000
        env:
        - name: ENVIRONMENT
          value: "production"
        - name: REDIS_URL
          valueFrom:
            secretKeyRef:
              name: legallink-secrets
              key: redis-url
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 10
```

### Monitoring Setup

#### Prometheus Metrics
```python
# Metrics collection
from prometheus_client import Counter, Histogram, Gauge

# Define metrics
message_counter = Counter('legallink_messages_total', 'Total messages processed')
response_time = Histogram('legallink_response_time_seconds', 'Response time')
active_connections = Gauge('legallink_active_connections', 'Active WebSocket connections')

# Middleware for metrics
@app.middleware("http")
async def metrics_middleware(request: Request, call_next):
    start_time = time.time()
    response = await call_next(request)
    process_time = time.time() - start_time
    
    response_time.observe(process_time)
    message_counter.inc()
    
    return response
```

#### Logging Configuration
```python
# Structured logging
import structlog

logger = structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    wrapper_class=structlog.stdlib.BoundLogger,
    cache_logger_on_first_use=True,
)
```

---

## üõ†Ô∏è Troubleshooting

### Common Issues

#### WebSocket Connection Problems
```python
# Debug WebSocket issues
async def debug_websocket_connection(websocket: WebSocket, user_id: str):
    try:
        await websocket.accept()
        logger.info(f"WebSocket connected for user {user_id}")
        
        # Send test message
        await websocket.send_json({
            "type": "connection_test",
            "message": "Connection established successfully"
        })
        
    except Exception as e:
        logger.error(f"WebSocket connection failed for user {user_id}: {e}")
        await websocket.close(code=1011, reason="Internal server error")
```

**Solutions:**
1. Check CORS configuration
2. Verify WebSocket URL format
3. Check firewall settings
4. Validate SSL certificates

#### Vector Database Issues
```python
# Vector DB health check
async def check_vector_db_health():
    try:
        # Test collection access
        collection = vector_db.get_collection("legal_knowledge")
        count = collection.count()
        
        return {
            "status": "healthy",
            "document_count": count,
            "last_updated": get_last_update_time()
        }
    except Exception as e:
        logger.error(f"Vector DB health check failed: {e}")
        return {"status": "unhealthy", "error": str(e)}
```

**Solutions:**
1. Restart ChromaDB service
2. Check data directory permissions
3. Verify embedding model availability
4. Re-index training data if corrupted

#### Ollama Integration Issues
```python
# Ollama service diagnostics
async def diagnose_ollama_issues():
    try:
        # Check server connectivity
        response = await ollama_client.get("/api/tags")
        models = response.json()
        
        # Check model availability
        if "gemma3" not in [m["name"] for m in models["models"]]:
            await ollama_client.pull_model("gemma3")
        
        return {"status": "healthy", "models": models}
    except Exception as e:
        logger.error(f"Ollama diagnostics failed: {e}")
        return {"status": "unhealthy", "error": str(e)}
```

### Performance Issues

#### High Response Times
```python
# Performance profiling
async def profile_response_time(query: str):
    start_time = time.time()
    
    # Vector search timing
    vector_start = time.time()
    context = await vector_db.similarity_search(query)
    vector_time = time.time() - vector_start
    
    # AI processing timing
    ai_start = time.time()
    response = await ollama_service.generate_response(query, context)
    ai_time = time.time() - ai_start
    
    total_time = time.time() - start_time
    
    logger.info(f"Performance profile - Total: {total_time:.2f}s, Vector: {vector_time:.2f}s, AI: {ai_time:.2f}s")
    
    return {
        "total_time": total_time,
        "vector_search_time": vector_time,
        "ai_processing_time": ai_time
    }
```

#### Memory Usage Optimization
```python
# Memory monitoring
import psutil

def monitor_memory_usage():
    process = psutil.Process()
    memory_info = process.memory_info()
    
    return {
        "rss": memory_info.rss / 1024 / 1024,  # MB
        "vms": memory_info.vms / 1024 / 1024,  # MB
        "percent": process.memory_percent()
    }

# Cleanup operations
async def cleanup_resources():
    """Periodic cleanup of resources"""
    # Clear old sessions
    await session_manager.cleanup_expired_sessions()
    
    # Clear cache entries
    await cache_manager.cleanup_expired_cache()
    
    # Garbage collection
    import gc
    gc.collect()
```

### Debug Mode

#### Verbose Logging
```python
# Enable debug logging
if os.getenv("DEBUG", "false").lower() == "true":
    logging.basicConfig(level=logging.DEBUG)
    
    # Log all WebSocket messages
    @app.middleware("websocket")
    async def log_websocket_messages(websocket: WebSocket, call_next):
        logger.debug(f"WebSocket message received from {websocket.client}")
        return await call_next(websocket)
```

#### Test Mode
```python
# Test configuration
class TestConfig:
    def __init__(self):
        self.use_mock_services = True
        self.bypass_rate_limits = True
        self.enable_debug_endpoints = True

@app.get("/debug/conversation/{session_id}")
async def debug_conversation(session_id: str):
    """Debug endpoint for conversation analysis"""
    if not app.config.enable_debug_endpoints:
        raise HTTPException(status_code=404)
    
    conversation = await session_manager.get_conversation_debug_info(session_id)
    return conversation
```

---

## üìà Future Enhancements

### Planned Features

#### Advanced AI Capabilities
- **Multi-modal Support**: Document analysis with OCR
- **Voice Integration**: Speech-to-text and text-to-speech
- **Regional Language Models**: Fine-tuned models for Indian languages
- **Legal Document Generation**: Automated legal document creation

#### Enhanced Integrations
- **Court Management Systems**: Integration with e-court platforms
- **Payment Gateway**: Integrated consultation payments
- **Video Consultation**: Built-in video calling for consultations
- **Document Management**: Legal document storage and sharing

#### Analytics & Intelligence
- **Predictive Analytics**: Case outcome predictions
- **Legal Trend Analysis**: Market intelligence for legal trends
- **Performance Analytics**: Advocate performance tracking
- **User Behavior Analysis**: Improved conversation flows

### Roadmap

#### Phase 1 (Q2 2024)
- Enhanced training data coverage
- Improved conversation flows
- Advanced advocate matching
- Mobile app integration

#### Phase 2 (Q3 2024)
- Multi-modal capabilities
- Voice integration
- Payment system integration
- Advanced analytics dashboard

#### Phase 3 (Q4 2024)
- Predictive analytics
- Regional language models
- Video consultation platform
- Enterprise features

---

## ü§ù Contributing

### Development Guidelines

#### Code Standards
- **Python**: Follow PEP 8 style guidelines
- **Type Hints**: Add type hints to all functions
- **Documentation**: Comprehensive docstrings for all classes and methods
- **Testing**: Unit tests for all new features

#### Contribution Process
1. Fork the repository
2. Create a feature branch
3. Implement changes with tests
4. Submit pull request with detailed description
5. Code review and feedback integration

### Testing

#### Unit Tests
```python
# test_conversation_orchestrator.py
import pytest
from app.agents import ConversationOrchestrator

@pytest.mark.asyncio
async def test_legal_query_processing():
    orchestrator = ConversationOrchestrator()
    
    query = "I need help with property dispute"
    user_context = {"location": "Mumbai", "language": "english"}
    
    response = await orchestrator.process_message(
        user_id="test_user",
        message_data={"message": query},
        user_context=user_context
    )
    
    assert response["classification"]["category"] == "property_law"
    assert len(response["advocate_matches"]) > 0
```

#### Integration Tests
```python
# test_api_integration.py
@pytest.mark.asyncio
async def test_websocket_conversation():
    async with websockets.connect("ws://localhost:8000/ws/chat/test_user") as websocket:
        # Send test message
        await websocket.send(json.dumps({
            "type": "message",
            "content": "Hello, I need legal help"
        }))
        
        # Receive response
        response = await websocket.recv()
        data = json.loads(response)
        
        assert data["type"] == "response"
        assert "content" in data
```

---

**This comprehensive documentation provides a complete overview of the LegalLink AI Query Assistant, covering all aspects from architecture to deployment. The system represents a cutting-edge approach to legal technology, combining advanced AI capabilities with practical legal assistance features.**

**For additional support, please refer to the GitHub repository, technical documentation, or contact the development team.**

---

*¬© 2024 LegalLink AI. All rights reserved. Built with ‚ù§Ô∏è for legal accessibility in India.*
